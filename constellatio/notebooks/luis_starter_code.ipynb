{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/marko/data/ttt.txt', 'r') as f:\n",
    "    selfies = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/home/marko/gselfies/mofid_llm_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/home/marko/.cache/huggingface/datasets/text/default-ec4e2832e7949f97/0.0.0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f8003c561e4009ab61fbf559eab7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60013c4541d04f92a207b7f60cfd457e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "datasets = Dataset.from_text('/home/marko/data/ttt.txt')\n",
    "datasets.train_test_split(test_size=0.2).save_to_disk('test_datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict.load_from_disk('test_datasets')\n",
    "trainset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['[C] [:0benzene] [Branch] [N] [Branch] [C] [C] [=N] [C] [C] [N] [Ring1] [Branch] [pop] [:0benzene] [Ring2] [O] [pop] [pop] ',\n",
       "  '[C] [N] [Branch] [C] [pop] [C] [=Branch] [=O] [pop] [N] [:0benzene] [Ring2] [Cl] [pop] [Ring1] [Cl] [pop] ',\n",
       "  '[C] [O] [:0benzene] [Branch] [S] [=Branch] [=O] [pop] [=Branch] [=O] [pop] [N] [C] [C] [N] [Branch] [C] [C] [Branch] [C] [=Branch] [=O] [pop] [N] [O] [pop] [Ring1] [=Branch] [pop] [C] [=Branch] [=O] [pop] [O] [C] [:0benzene] [pop] ']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def my_collator(examples):\n",
    "    output = tokenizer(\n",
    "        [e['text'] for e in examples],\n",
    "        truncation=True,\n",
    "        max_length=40,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "    )\n",
    "    # output['labels'] = torch.tensor([e['label'] for e in examples])\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "tloader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=my_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "_iter = iter(tloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 40])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2Model\n",
    "from ml_collections import ConfigDict\n",
    "cfg = ConfigDict()\n",
    "cfg.gptcfg = gpt2cfg = ConfigDict()\n",
    "gpt2cfg.n_embd = 32\n",
    "gpt2cfg.n_layer = 4\n",
    "gpt2cfg.n_head = 4\n",
    "gpt2cfg.vocab_size = tokenizer.vocab_size\n",
    "gpt2cfg.n_positions = 40\n",
    "gpt2cfg = GPT2Config(**{k:v for k,v in cfg.gptcfg.items() if k in GPT2Config().to_dict()})\n",
    "model = GPT2Model(gpt2cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "emb2nrg = nn.Linear(32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 40])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model(**batch)[0][:,0,:]\n",
    "nrg = emb2nrg(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from transformers import GPT2Model\n",
    "from transformers import GPT2Config\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "cfg.lr = 1e-3\n",
    "cfg.warmup_steps = 1000\n",
    "class NRGPredictor(pl.LightningModule):\n",
    "    def __init__(self, cfg, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cfg = cfg\n",
    "        self.save_hyperparameters()\n",
    "        gpt2cfg = GPT2Config(**{k:v for k,v in cfg.gptcfg.items() if k in GPT2Config().to_dict()})\n",
    "        self.model = GPT2Model(gpt2cfg)\n",
    "        self.emb2nrg = nn.Linear(32,1)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "\n",
    "    def forward(self, **batch):\n",
    "        emb = self.model(**batch)[0][:,0,:]\n",
    "        nrg = self.emb2nrg(emb)\n",
    "        return nrg\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs.mean()\n",
    "        self.log(\"train_loss\", loss, on_step=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs.mean()\n",
    "        self.log(\"val_loss\", loss, on_step=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.cfg.lr)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, self.cfg.warmup_steps, self.cfg.warmup_steps*5)\n",
    "        return [optimizer], {\"scheduler\": scheduler, \"interval\": \"step\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] =\"xxx\"\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project='test',\n",
    "    name='asdadfasff',\n",
    "    log_model=True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger = wandb_logger,\n",
    "    accelerator='auto',\n",
    "    gradient_clip_algorithm='norm',\n",
    "    gradient_clip_val=1.0,\n",
    "    devices=1,\n",
    "    max_epochs=10,\n",
    "    check_val_every_n_epoch=1,\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            patience=10,\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval='step'),\n",
    "        # ModelCheckpoint(\n",
    "        #     dirpath=f'{setupparams.base_dir}/{setupparams.experiment_name}_checkpoints/{run_name}',\n",
    "        #     monitor='val_loss',\n",
    "        #     every_n_epochs=1,\n",
    "        #     save_top_k=2,\n",
    "        # ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marko/miniconda3/envs/ringmaster/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name    | Type      | Params\n",
      "--------------------------------------\n",
      "0 | model   | GPT2Model | 71.7 K\n",
      "1 | emb2nrg | Linear    | 33    \n",
      "2 | loss_fn | MSELoss   | 0     \n",
      "--------------------------------------\n",
      "71.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "71.7 K    Total params\n",
      "0.287     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6cbc186147412ea82c88a4e1cb9fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marko/miniconda3/envs/ringmaster/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "model = NRGPredictor(cfg, tokenizer)\n",
    "trainer.fit(model, tloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constellatio.datasets.pubchemqc import random_cids, get_cids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cids = get_cids(1000, offset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = [c for c in cids]\n",
    "aaaa = aaa[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def fetch_molecule_data(cids):\n",
    "    base_url = \"https://pcqc.matter.toronto.edu/pm6opt_chon300nosalt\"\n",
    "    molecule_data_list = []\n",
    "\n",
    "    # Convert the list of CIDs into a comma-separated string\n",
    "    cid_list = \",\".join(map(str, cids))\n",
    "\n",
    "    # Use the `any` operator in the query parameters\n",
    "    params = {\"select\": \"*\", \"and\": f\"(cid.any.{cid_list})\"}\n",
    "\n",
    "    # Debugging: print out the final URL and parameters\n",
    "    print(\"Requesting URL:\", base_url)\n",
    "    print(\"With parameters:\", params)\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        molecule_data = response.json()\n",
    "        if molecule_data:  # Check if data is not empty\n",
    "            molecule_data_list.extend(molecule_data)  # Add all fetched records to the list\n",
    "    else:\n",
    "        print(f\"Failed to fetch data with status code {response.status_code}: {response.text}\")\n",
    "\n",
    "    return molecule_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting URL: https://pcqc.matter.toronto.edu/pm6opt_chon300nosalt\n",
      "With parameters: {'select': '*', 'and': '(cid.eq(any).3,4,7,8,12,15,16,17,18)'}\n",
      "Failed to fetch data with status code 400: {\"code\":\"PGRST100\",\"details\":\"unexpected \\\",\\\" expecting letter, digit, \\\"-\\\", \\\"->>\\\", \\\"->\\\" or delimiter (.)\",\"hint\":null,\"message\":\"\\\"failed to parse logic tree ((cid.eq(any).3,4,7,8,12,15,16,17,18))\\\" (line 1, column 20)\"}\n"
     ]
    }
   ],
   "source": [
    "data = fetch_molecule_data(aaaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 12,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 25,\n",
       " 26,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 32,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 42,\n",
       " 43,\n",
       " 45,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 58,\n",
       " 63,\n",
       " 67,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 79,\n",
       " 80,\n",
       " 86,\n",
       " 87,\n",
       " 91,\n",
       " 93,\n",
       " 95,\n",
       " 96,\n",
       " 101,\n",
       " 102,\n",
       " 104,\n",
       " 107,\n",
       " 108,\n",
       " 111,\n",
       " 114,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 135,\n",
       " 137,\n",
       " 138,\n",
       " 142,\n",
       " 144,\n",
       " 145,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 166,\n",
       " 167,\n",
       " 171,\n",
       " 173,\n",
       " 174,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 185,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 196,\n",
       " 199,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 211,\n",
       " 215,\n",
       " 218,\n",
       " 219,\n",
       " 221,\n",
       " 222,\n",
       " 225,\n",
       " 227,\n",
       " 229,\n",
       " 232,\n",
       " 236,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 243,\n",
       " 244,\n",
       " 247,\n",
       " 252,\n",
       " 254,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 273,\n",
       " 275,\n",
       " 277,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 284,\n",
       " 286,\n",
       " 288,\n",
       " 289,\n",
       " 296,\n",
       " 297,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 320,\n",
       " 322,\n",
       " 323,\n",
       " 325,\n",
       " 326,\n",
       " 328,\n",
       " 331,\n",
       " 332,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 340,\n",
       " 342,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 349,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 358,\n",
       " 359,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 366,\n",
       " 368,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 374,\n",
       " 376,\n",
       " 379,\n",
       " 384,\n",
       " 385,\n",
       " 389,\n",
       " 392,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 403,\n",
       " 404,\n",
       " 408,\n",
       " 409,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 418,\n",
       " 419,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 427,\n",
       " 428,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 441,\n",
       " 443,\n",
       " 449,\n",
       " 450,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 460,\n",
       " 463,\n",
       " 464,\n",
       " 469,\n",
       " 470,\n",
       " 472,\n",
       " 484,\n",
       " 487,\n",
       " 496,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 512,\n",
       " 513,\n",
       " 515,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 527,\n",
       " 528,\n",
       " 535,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 547,\n",
       " 548,\n",
       " 550,\n",
       " 551,\n",
       " 553,\n",
       " 554,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 564,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 588,\n",
       " 589,\n",
       " 596,\n",
       " 597,\n",
       " 599,\n",
       " 601,\n",
       " 602,\n",
       " 604,\n",
       " 607,\n",
       " 608,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 614,\n",
       " 617,\n",
       " 619,\n",
       " 626,\n",
       " 627,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 643,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 652,\n",
       " 659,\n",
       " 660,\n",
       " 662,\n",
       " 670,\n",
       " 671,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 677,\n",
       " 681,\n",
       " 682,\n",
       " 690,\n",
       " 691,\n",
       " 694,\n",
       " 698,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 709,\n",
       " 712,\n",
       " 713,\n",
       " 716,\n",
       " 722,\n",
       " 723,\n",
       " 725,\n",
       " 736,\n",
       " 738,\n",
       " 739,\n",
       " 742,\n",
       " 743,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 756,\n",
       " 757,\n",
       " 760,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 767,\n",
       " 768,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 779,\n",
       " 780,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 787,\n",
       " 790,\n",
       " 791,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 798,\n",
       " 800,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 809,\n",
       " 811,\n",
       " 817,\n",
       " 824,\n",
       " 825,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 833,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 844,\n",
       " 846,\n",
       " 848,\n",
       " 849,\n",
       " 852,\n",
       " 854,\n",
       " 855,\n",
       " 857,\n",
       " 858,\n",
       " 860,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 880,\n",
       " 887,\n",
       " 891,\n",
       " 892,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 899,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 907,\n",
       " 910,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 931,\n",
       " 932,\n",
       " 936,\n",
       " 938,\n",
       " 942,\n",
       " 944,\n",
       " 945,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 954,\n",
       " 955,\n",
       " 957,\n",
       " 962,\n",
       " 964,\n",
       " 965,\n",
       " 970,\n",
       " 971,\n",
       " 972,\n",
       " 974,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 984,\n",
       " 985,\n",
       " 988,\n",
       " 989,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999,\n",
       " 1000,\n",
       " 1001,\n",
       " 1002,\n",
       " 1017,\n",
       " 1018,\n",
       " 1021,\n",
       " 1028,\n",
       " 1029,\n",
       " 1030,\n",
       " 1031,\n",
       " 1032,\n",
       " 1043,\n",
       " 1044,\n",
       " 1045,\n",
       " 1046,\n",
       " 1047,\n",
       " 1048,\n",
       " 1049,\n",
       " 1050,\n",
       " 1052,\n",
       " 1054,\n",
       " 1057,\n",
       " 1059,\n",
       " 1060,\n",
       " 1064,\n",
       " 1066,\n",
       " 1070,\n",
       " 1071,\n",
       " 1081,\n",
       " 1087,\n",
       " 1088,\n",
       " 1094,\n",
       " 1101,\n",
       " 1102,\n",
       " 1103,\n",
       " 1104,\n",
       " 1107,\n",
       " 1108,\n",
       " 1110,\n",
       " 1112,\n",
       " 1114,\n",
       " 1122,\n",
       " 1125,\n",
       " 1134,\n",
       " 1135,\n",
       " 1140,\n",
       " 1141,\n",
       " 1145,\n",
       " 1146,\n",
       " 1148,\n",
       " 1150,\n",
       " 1153,\n",
       " 1174,\n",
       " 1175,\n",
       " 1176,\n",
       " 1177,\n",
       " 1178,\n",
       " 1182,\n",
       " 1183,\n",
       " 1188,\n",
       " 1189,\n",
       " 1194,\n",
       " 1196,\n",
       " 1197,\n",
       " 1198,\n",
       " 1203,\n",
       " 1206,\n",
       " 1207,\n",
       " 1208,\n",
       " 1209,\n",
       " 1212,\n",
       " 1213,\n",
       " 1214,\n",
       " 1217,\n",
       " 1219,\n",
       " 1220,\n",
       " 1221,\n",
       " 1222,\n",
       " 1226,\n",
       " 1227,\n",
       " 1232,\n",
       " 1233,\n",
       " 1235,\n",
       " 1237,\n",
       " 1241,\n",
       " 1242,\n",
       " 1245,\n",
       " 1250,\n",
       " 1253,\n",
       " 1254,\n",
       " 1255,\n",
       " 1256,\n",
       " 1257,\n",
       " 1271,\n",
       " 1274,\n",
       " 1282,\n",
       " 1292,\n",
       " 1295,\n",
       " 1297,\n",
       " 1302,\n",
       " 1303,\n",
       " 1307,\n",
       " 1310,\n",
       " 1313,\n",
       " 1317,\n",
       " 1318,\n",
       " 1320,\n",
       " 1322,\n",
       " 1326,\n",
       " 1328,\n",
       " 1334,\n",
       " 1335,\n",
       " 1336,\n",
       " 1340,\n",
       " 1342,\n",
       " 1343,\n",
       " 1344,\n",
       " 1346,\n",
       " 1347,\n",
       " 1366,\n",
       " 1367,\n",
       " 1368,\n",
       " 1370,\n",
       " 1373,\n",
       " 1374,\n",
       " 1382,\n",
       " 1383,\n",
       " 1384,\n",
       " 1388,\n",
       " 1390,\n",
       " 1391,\n",
       " 1400,\n",
       " 1414,\n",
       " 1416,\n",
       " 1420,\n",
       " 1424,\n",
       " 1425,\n",
       " 1429,\n",
       " 1433,\n",
       " 1446,\n",
       " 1447,\n",
       " 1449,\n",
       " 1456,\n",
       " 1457,\n",
       " 1465,\n",
       " 1468,\n",
       " 1472,\n",
       " 1474,\n",
       " 1484,\n",
       " 1491,\n",
       " 1493,\n",
       " 1494,\n",
       " 1508,\n",
       " 1511,\n",
       " 1515,\n",
       " 1522,\n",
       " 1527,\n",
       " 1530,\n",
       " 1531,\n",
       " 1532,\n",
       " 1533,\n",
       " 1539,\n",
       " 1551,\n",
       " 1553,\n",
       " 1561,\n",
       " 1562,\n",
       " 1563,\n",
       " 1574,\n",
       " 1576,\n",
       " 1584,\n",
       " 1599,\n",
       " 1606,\n",
       " 1610,\n",
       " 1614,\n",
       " 1615,\n",
       " 1616,\n",
       " 1621,\n",
       " 1625,\n",
       " 1626,\n",
       " 1628,\n",
       " 1639,\n",
       " 1642,\n",
       " 1645,\n",
       " 1647,\n",
       " 1650,\n",
       " 1651,\n",
       " 1652,\n",
       " 1653,\n",
       " 1656,\n",
       " 1662,\n",
       " 1663,\n",
       " 1665,\n",
       " 1667,\n",
       " 1669,\n",
       " 1670,\n",
       " 1671,\n",
       " 1672,\n",
       " 1673,\n",
       " 1674,\n",
       " 1675,\n",
       " 1676,\n",
       " 1678,\n",
       " 1679,\n",
       " 1681,\n",
       " 1685,\n",
       " 1693,\n",
       " 1695,\n",
       " 1698,\n",
       " 1699,\n",
       " 1706,\n",
       " 1708,\n",
       " 1720,\n",
       " 1721,\n",
       " 1725,\n",
       " 1727,\n",
       " 1733,\n",
       " 1737,\n",
       " 1738,\n",
       " 1741,\n",
       " 1742,\n",
       " 1743,\n",
       " 1745,\n",
       " 1748,\n",
       " 1750,\n",
       " 1752,\n",
       " 1754,\n",
       " 1755,\n",
       " 1756,\n",
       " 1774,\n",
       " 1775,\n",
       " 1776,\n",
       " 1780,\n",
       " 1801,\n",
       " 1802,\n",
       " 1803,\n",
       " 1805,\n",
       " 1809,\n",
       " 1814,\n",
       " 1825,\n",
       " 1826,\n",
       " 1828,\n",
       " 1832,\n",
       " 1833,\n",
       " 1835,\n",
       " 1837,\n",
       " 1839,\n",
       " 1840,\n",
       " 1844,\n",
       " 1853,\n",
       " 1855,\n",
       " 1861,\n",
       " 1863,\n",
       " 1864,\n",
       " 1865,\n",
       " 1868,\n",
       " 1869,\n",
       " 1870,\n",
       " 1871,\n",
       " 1879,\n",
       " 1880,\n",
       " 1881,\n",
       " 1889,\n",
       " 1890,\n",
       " 1892,\n",
       " 1893,\n",
       " 1894,\n",
       " 1900,\n",
       " 1917,\n",
       " 1921,\n",
       " 1922,\n",
       " 1923,\n",
       " 1927,\n",
       " 1929,\n",
       " 1930,\n",
       " 1931,\n",
       " 1932,\n",
       " 1933,\n",
       " 1935,\n",
       " 1936,\n",
       " 1938,\n",
       " 1942,\n",
       " 1962,\n",
       " 1971,\n",
       " 1979,\n",
       " 1980,\n",
       " 1983,\n",
       " 1984,\n",
       " 1990,\n",
       " 1995,\n",
       " 1999,\n",
       " 2000,\n",
       " 2002,\n",
       " 2003,\n",
       " 2004,\n",
       " 2005,\n",
       " 2006,\n",
       " 2015,\n",
       " 2017,\n",
       " 2018,\n",
       " 2022,\n",
       " 2041,\n",
       " 2042,\n",
       " 2045,\n",
       " 2046,\n",
       " 2048,\n",
       " 2049,\n",
       " 2052,\n",
       " 2054,\n",
       " 2055,\n",
       " 2057,\n",
       " 2061,\n",
       " 2063,\n",
       " 2064,\n",
       " 2065,\n",
       " 2068,\n",
       " 2071,\n",
       " 2072,\n",
       " 2079,\n",
       " 2080,\n",
       " 2083,\n",
       " 2093,\n",
       " 2094,\n",
       " 2095,\n",
       " 2097,\n",
       " 2099,\n",
       " 2101,\n",
       " 2102,\n",
       " 2105,\n",
       " 2107,\n",
       " 2108,\n",
       " 2109,\n",
       " 2110,\n",
       " 2112,\n",
       " 2113,\n",
       " 2119,\n",
       " 2120,\n",
       " 2123,\n",
       " 2130,\n",
       " 2136,\n",
       " 2138,\n",
       " 2145,\n",
       " 2146,\n",
       " 2147,\n",
       " 2148,\n",
       " 2149,\n",
       " 2151,\n",
       " 2153,\n",
       " 2160,\n",
       " 2161,\n",
       " 2164,\n",
       " 2172,\n",
       " 2181,\n",
       " 2187,\n",
       " 2192,\n",
       " 2193,\n",
       " 2196,\n",
       " 2197,\n",
       " 2199,\n",
       " 2200,\n",
       " 2201,\n",
       " 2202,\n",
       " 2206,\n",
       " 2214,\n",
       " 2215,\n",
       " 2227,\n",
       " 2229,\n",
       " 2230,\n",
       " 2234,\n",
       " 2235,\n",
       " 2237,\n",
       " 2240,\n",
       " 2242,\n",
       " 2244,\n",
       " 2249,\n",
       " 2252,\n",
       " 2253,\n",
       " 2266,\n",
       " 2272,\n",
       " 2277,\n",
       " 2278,\n",
       " 2285,\n",
       " 2292,\n",
       " 2294,\n",
       " 2295,\n",
       " 2309,\n",
       " 2310,\n",
       " 2312,\n",
       " 2313,\n",
       " 2314,\n",
       " 2327,\n",
       " 2331,\n",
       " 2332,\n",
       " 2336,\n",
       " 2337,\n",
       " 2340,\n",
       " 2341,\n",
       " 2345,\n",
       " 2354,\n",
       " 2355,\n",
       " 2358,\n",
       " 2360,\n",
       " 2361,\n",
       " 2365,\n",
       " 2366,\n",
       " 2368,\n",
       " 2374,\n",
       " 2377,\n",
       " 2380,\n",
       " 2383,\n",
       " 2390,\n",
       " 2409,\n",
       " 2420,\n",
       " 2430,\n",
       " 2438,\n",
       " 2457,\n",
       " 2461,\n",
       " 2463,\n",
       " 2466,\n",
       " 2468,\n",
       " 2473,\n",
       " 2474,\n",
       " 2479,\n",
       " 2481,\n",
       " 2482,\n",
       " 2483,\n",
       " 2515,\n",
       " 2518,\n",
       " 2519,\n",
       " 2527,\n",
       " 2535,\n",
       " 2537,\n",
       " 2544,\n",
       " 2545,\n",
       " 2553,\n",
       " 2554,\n",
       " 2555,\n",
       " 2563,\n",
       " 2566,\n",
       " 2568,\n",
       " 2570,\n",
       " 2572,\n",
       " 2574,\n",
       " 2576,\n",
       " 2583,\n",
       " 2592,\n",
       " 2595,\n",
       " 2598,\n",
       " 2599,\n",
       " 2600,\n",
       " 2602,\n",
       " 2604,\n",
       " 2606,\n",
       " 2677,\n",
       " 2682,\n",
       " 2687,\n",
       " 2704,\n",
       " 2741,\n",
       " 2743,\n",
       " 2746,\n",
       " 2747,\n",
       " 2749,\n",
       " 2755,\n",
       " 2757,\n",
       " 2758,\n",
       " 2762,\n",
       " 2765,\n",
       " 2766,\n",
       " 2775,\n",
       " 2779,\n",
       " 2828,\n",
       " 2859,\n",
       " 2877,\n",
       " 2879,\n",
       " 2883,\n",
       " 2890,\n",
       " 2893,\n",
       " 2895,\n",
       " 2896,\n",
       " 2897,\n",
       " 2899,\n",
       " 2900,\n",
       " 2901,\n",
       " 2902,\n",
       " 2903,\n",
       " 2905,\n",
       " 2906,\n",
       " 2911,\n",
       " 2913,\n",
       " 2930,\n",
       " 2933,\n",
       " 2944,\n",
       " 2946,\n",
       " 2947,\n",
       " 2948,\n",
       " 2950,\n",
       " 2954,\n",
       " 2960,\n",
       " 2963,\n",
       " 2969,\n",
       " 2970,\n",
       " 2972,\n",
       " 2980,\n",
       " 2981,\n",
       " 2985,\n",
       " 2986,\n",
       " 2992,\n",
       " 2995,\n",
       " 2998,\n",
       " 3007,\n",
       " 3008,\n",
       " 3022,\n",
       " 3026,\n",
       " 3043,\n",
       " 3044,\n",
       " 3045,\n",
       " 3049,\n",
       " 3051,\n",
       " 3052,\n",
       " 3054,\n",
       " 3071,\n",
       " 3079,\n",
       " 3081,\n",
       " 3086,\n",
       " 3090,\n",
       " 3092,\n",
       " 3094,\n",
       " 3097,\n",
       " 3100,\n",
       " 3102,\n",
       " 3103,\n",
       " 3106,\n",
       " 3121,\n",
       " 3124,\n",
       " 3125,\n",
       " 3134,\n",
       " 3135,\n",
       " 3136,\n",
       " 3139,\n",
       " 3140]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ringmaster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
