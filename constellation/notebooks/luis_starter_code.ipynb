{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/marko/data/ttt.txt', 'r') as f:\n",
    "    selfies = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/home/marko/gselfies/mofid_llm_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/home/marko/.cache/huggingface/datasets/text/default-ec4e2832e7949f97/0.0.0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f8003c561e4009ab61fbf559eab7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60013c4541d04f92a207b7f60cfd457e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "datasets = Dataset.from_text('/home/marko/data/ttt.txt')\n",
    "datasets.train_test_split(test_size=0.2).save_to_disk('test_datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict.load_from_disk('test_datasets')\n",
    "trainset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['[C] [:0benzene] [Branch] [N] [Branch] [C] [C] [=N] [C] [C] [N] [Ring1] [Branch] [pop] [:0benzene] [Ring2] [O] [pop] [pop] ',\n",
       "  '[C] [N] [Branch] [C] [pop] [C] [=Branch] [=O] [pop] [N] [:0benzene] [Ring2] [Cl] [pop] [Ring1] [Cl] [pop] ',\n",
       "  '[C] [O] [:0benzene] [Branch] [S] [=Branch] [=O] [pop] [=Branch] [=O] [pop] [N] [C] [C] [N] [Branch] [C] [C] [Branch] [C] [=Branch] [=O] [pop] [N] [O] [pop] [Ring1] [=Branch] [pop] [C] [=Branch] [=O] [pop] [O] [C] [:0benzene] [pop] ']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def my_collator(examples):\n",
    "    output = tokenizer(\n",
    "        [e['text'] for e in examples],\n",
    "        truncation=True,\n",
    "        max_length=40,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "    )\n",
    "    # output['labels'] = torch.tensor([e['label'] for e in examples])\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "tloader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=my_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "_iter = iter(tloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 40])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2Model\n",
    "from ml_collections import ConfigDict\n",
    "cfg = ConfigDict()\n",
    "cfg.gptcfg = gpt2cfg = ConfigDict()\n",
    "gpt2cfg.n_embd = 32\n",
    "gpt2cfg.n_layer = 4\n",
    "gpt2cfg.n_head = 4\n",
    "gpt2cfg.vocab_size = tokenizer.vocab_size\n",
    "gpt2cfg.n_positions = 40\n",
    "gpt2cfg = GPT2Config(**{k:v for k,v in cfg.gptcfg.items() if k in GPT2Config().to_dict()})\n",
    "model = GPT2Model(gpt2cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "emb2nrg = nn.Linear(32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 40])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model(**batch)[0][:,0,:]\n",
    "nrg = emb2nrg(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from transformers import GPT2Model\n",
    "from transformers import GPT2Config\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "cfg.lr = 1e-3\n",
    "cfg.warmup_steps = 1000\n",
    "class NRGPredictor(pl.LightningModule):\n",
    "    def __init__(self, cfg, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cfg = cfg\n",
    "        self.save_hyperparameters()\n",
    "        gpt2cfg = GPT2Config(**{k:v for k,v in cfg.gptcfg.items() if k in GPT2Config().to_dict()})\n",
    "        self.model = GPT2Model(gpt2cfg)\n",
    "        self.emb2nrg = nn.Linear(32,1)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "\n",
    "    def forward(self, **batch):\n",
    "        emb = self.model(**batch)[0][:,0,:]\n",
    "        nrg = self.emb2nrg(emb)\n",
    "        return nrg\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs.mean()\n",
    "        self.log(\"train_loss\", loss, on_step=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs.mean()\n",
    "        self.log(\"val_loss\", loss, on_step=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.cfg.lr)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, self.cfg.warmup_steps, self.cfg.warmup_steps*5)\n",
    "        return [optimizer], {\"scheduler\": scheduler, \"interval\": \"step\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] =\"xxx\"\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project='test',\n",
    "    name='asdadfasff',\n",
    "    log_model=True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger = wandb_logger,\n",
    "    accelerator='auto',\n",
    "    gradient_clip_algorithm='norm',\n",
    "    gradient_clip_val=1.0,\n",
    "    devices=1,\n",
    "    max_epochs=10,\n",
    "    check_val_every_n_epoch=1,\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            patience=10,\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval='step'),\n",
    "        # ModelCheckpoint(\n",
    "        #     dirpath=f'{setupparams.base_dir}/{setupparams.experiment_name}_checkpoints/{run_name}',\n",
    "        #     monitor='val_loss',\n",
    "        #     every_n_epochs=1,\n",
    "        #     save_top_k=2,\n",
    "        # ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marko/miniconda3/envs/ringmaster/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name    | Type      | Params\n",
      "--------------------------------------\n",
      "0 | model   | GPT2Model | 71.7 K\n",
      "1 | emb2nrg | Linear    | 33    \n",
      "2 | loss_fn | MSELoss   | 0     \n",
      "--------------------------------------\n",
      "71.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "71.7 K    Total params\n",
      "0.287     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6cbc186147412ea82c88a4e1cb9fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marko/miniconda3/envs/ringmaster/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "model = NRGPredictor(cfg, tokenizer)\n",
    "trainer.fit(model, tloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ringmaster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
